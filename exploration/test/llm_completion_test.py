from google import genai
from google.genai import types
import re,os
import networkx as nx
import spark_dsg as dsg
from pathlib import Path
import spark_dsg.networkx as dsg_nx
from bidict import bidict
import numpy as np
import yaml
import pathlib
import time
from scene_graph_visualize_2d import create_scene_slices
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

class FlowList(list):
    pass

def flow_list_representer(dumper, data):
    return dumper.represent_sequence('tag:yaml.org,2002:seq', data, flow_style=True)


class LLMCompletion:
    def __init__(self, base_path, file_to_analyze, model_name, user_prompt, seed):
        self.base_path = base_path
        yaml.SafeDumper.add_representer(FlowList, flow_list_representer)
        self.file_to_analyze = file_to_analyze
        self.model_name = model_name
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.user_prompt = user_prompt
        self.seed = seed

    def search_nearest_room_node(self, G, object_pos):
        room_layer = G.get_layer(dsg.DsgLayers.ROOMS)
        room_nodes = list(room_layer.nodes)
        best_parent = None
        best_distance = float('inf')
        for room_node in room_nodes:
            room_pos = room_node.attributes.position
            distance = np.linalg.norm(np.array(room_pos) - np.array(object_pos))
            if distance < best_distance:
                best_distance = distance
                best_parent = room_node.id.value
        return best_parent


    def preprocess_file(self, original_yaml_filename, node_mapping_filename):
        path_to_dsg = pathlib.Path(self.file_to_analyze).expanduser().absolute()

        # Load the scene graph
        G = dsg.DynamicSceneGraph.load(str(path_to_dsg))
        object_layer = G.get_layer(dsg.DsgLayers.OBJECTS)
        object_nodes = list(object_layer.nodes)
        for node in object_nodes:
            if node.attributes.name != "Nothing":
                place_parent = node.get_parent()
                if place_parent is None:
                    object_pos = node.attributes.position
                    room_parent = self.search_nearest_room_node(G, object_pos)
                else:
                    place_parent_node = G.get_node(place_parent)
                    room_parent = place_parent_node.get_parent()
                if room_parent is None:
                    object_pos = node.attributes.position
                    room_parent = self.search_nearest_room_node(G, object_pos)
                attr = dsg._dsg_bindings.EdgeAttributes()
                attr.weight = 1.0
                attr.weighted = False
                G.insert_edge(node.id.value, room_parent, attr)
            else:
                node.attributes.position = node.attributes.bounding_box.world_P_center

        # Convert the scene graph to a networkx graph
        easier_graph = dsg_nx.graph_to_networkx(G, include_dynamic=False)

        # Process the scene graph to find its minimal representation
        remove_nodes = []
        # Map new node IDs to original node IDs
        mapping = bidict()
        idx = 0

        # These numbers are generated by checking the first 3 digits of the node IDs in the scene graph. It may change if code changes.
        for node, attribute in easier_graph.nodes.items():
            # Segment Node
            if str(node)[:3] == '828':
                remove_nodes.append(node)
                            
            # Object Node
            elif str(node)[:3] == '799':
                new_id = idx
                mapping[new_id] = node
                idx += 1
                bounding_box_dim = attribute['bounding_box'].dimensions
                bounding_box_P = attribute['bounding_box'].world_P_center
                bounding_box_R = attribute['bounding_box'].world_R_center
                attribute['position'] = bounding_box_P
                attribute['dimension'] = bounding_box_dim
                attribute['orientation'] = bounding_box_R
                if attribute['name'] == 'Nothing':
                    attribute['node_type'] = 'nothing'
                elif (attribute['name'] == "Wall" or
                     attribute['name'] == "Window" or
                     attribute['name'] == "Curtain" or
                     attribute['name'] == "Blind" or
                     attribute['name'] == "Door"):
                    attribute['node_type'] = 'structure'
                else:
                    attribute['node_type'] = 'object'
                for key in list(attribute.keys()):
                    # Only preserve these attributes
                    if key == 'position' or key == 'node_type' or key == 'name' or key == 'is_predicted' or key == 'dimension' or key == 'orientation':
                        # Convert incompatible types to strings
                        if type(attribute[key]) == np.ndarray:
                            py_list = attribute[key].tolist()
                            if py_list and isinstance(py_list[0], list):
                                rounded_list = [[round(num, 3) for num in row] for row in py_list]
                            else:
                                rounded_list = [round(x, 3) for x in py_list]
                            attribute[key] = str(rounded_list)
                        elif type(attribute[key]) == list or type(attribute[key]) == dict:
                            attribute[key] = str(attribute[key])
                    else:
                        del attribute[key]

            # Place Node
            elif str(node)[:3] == '807':
                remove_nodes.append(node)

            # Room Node
            elif str(node)[:3] == '778':
                new_id = idx
                mapping[new_id] = node
                idx += 1
                attribute['node_type'] = 'room'
                for key in list(attribute.keys()):
                    # Only preserve these attributes
                    if key == 'name' or key == 'position' or key == 'node_type':
                        # Convert incompatible types to strings
                        if type(attribute[key]) == np.ndarray:
                            attribute[key] = str([round(x,3) for x in attribute[key].tolist()])
                        elif type(attribute[key]) == list or type(attribute[key]) == dict:
                            attribute[key] = str(attribute[key])
                    else:
                        del attribute[key]
            else:
                raise ValueError(f"Unexpected number of attributes for node {node}: {len(attribute)}")

        # Remove the nodes that we don't need
        for node in remove_nodes:
            easier_graph.remove_node(node)

        # Relabel the nodes with new IDs
        orig_to_new = dict(mapping.inverse)  # {orig_id: new_id}
        easier_graph = nx.relabel_nodes(easier_graph, orig_to_new)

        # Create a more compact YAML representation of the scene graph
        # 1. Create the list of nodes
        nodes_for_yaml = []
        for node_id, attributes in easier_graph.nodes(data=True):
            node_dict = {'id': node_id}  # Add the 'id' field
            node_dict.update(attributes) # Add all other attributes
            nodes_for_yaml.append(node_dict)

        # 2. Create the list of edges in the minimal [u, v] format
        edges_for_yaml = [FlowList(edge) for edge in easier_graph.edges()]

        data_to_dump = {
            'nodes': nodes_for_yaml,
            'edges': edges_for_yaml
        }

        # Write the data to a YAML file
        yaml_filepath = os.path.join(self.base_path, original_yaml_filename)
        with open(yaml_filepath, 'w') as yaml_file:
            yaml.safe_dump(data_to_dump, yaml_file, default_flow_style=False)

        mapping_dict = dict(mapping)
        # Define the new filepath for the mapping
        mapping_filepath = os.path.join(self.base_path, node_mapping_filename)
        # Write the mapping dictionary to its own YAML file
        with open(mapping_filepath, 'w') as mapping_file:
            yaml.safe_dump(mapping_dict, mapping_file)
    
    def _get_image_part(self, image_path):
        """Helper to create a Part from local image bytes for Gemini API."""
        with open(image_path, "rb") as f:
            image_bytes = f.read()
        return types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg')


    def generate_completion_response(self, yaml_filepath, image_dir):
        client = genai.Client(api_key=self.google_api_key)

        with open(yaml_filepath, 'r') as file:
            yaml_content = file.read()
        contents = [
            self.user_prompt,
            types.Part.from_text(text=yaml_content)
        ]

        # Base64 encoded images
        image_count = 0
        for img_path in Path(image_dir).iterdir():
            part = self._get_image_part(img_path)
            contents.append(part)
            image_count += 1

        total_tokens = client.models.count_tokens(
            model=self.model_name, contents=contents
        )
        print("Total input tokens: ", total_tokens)
        model_info = client.models.get(model=self.model_name)
        print(f"Input tokens limit: {model_info.input_token_limit=}")
        response = client.models.generate_content(
            model=self.model_name,
            contents=contents,
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=-1),
                seed = self.seed
            )
        )

        print("Received generation response.")
        # print(f"Generation Response: {response.text}")
        return response.text

    def parse_response(self, response_text, yaml_filepath, new_yaml_filename):
        # Remove the outside code block markers
        match = re.search(r"```(yaml\n)?(.*)```", response_text, re.DOTALL)
        clean_yaml_text = match.group(2).strip()

        clean_yaml_text = clean_yaml_text.replace(u'\xa0', u' ')

        # Fix the indentation of the YAML text (only issue I found)
        if "\n    nodes:" in clean_yaml_text:
            clean_yaml_text = clean_yaml_text.replace("\n    nodes:", "\nnodes:")

        # Load the cleaned YAML text and merge it with the original YAML file
        new_data = yaml.safe_load(clean_yaml_text)
        with open(yaml_filepath, 'r') as file:
            current_data = yaml.safe_load(file)
        current_data['nodes'].extend(new_data.get('nodes', []))
        current_data['edges'].extend(new_data.get('edges', []))
        current_data['edges'] = [FlowList(edge) for edge in current_data['edges']]
        new_yaml_filepath = os.path.join(self.base_path, new_yaml_filename)
        with open(new_yaml_filepath, 'w') as file:
            yaml.safe_dump(current_data, file, default_flow_style=False)

    def generate_refinement_response(self, user_prompt, new_items_yaml, original_yaml_filepath, image_dir):
        client = genai.Client(api_key=self.google_api_key)
        with open(original_yaml_filepath, 'r') as file:
            original_scene_content = file.read()

        # Construct a clear, structured input for the refinement model
        contents = [
            user_prompt,
            "## new_items:",
            new_items_yaml,
            "---",
            "## original_scene:",
            original_scene_content
        ]

        # Base64 encoded images
        image_count = 0
        for img_path in Path(image_dir).iterdir():
            part = self._get_image_part(img_path)
            contents.append(part)
            image_count += 1
        print(f"Uploaded {image_count} images to the model.")

        total_tokens = client.models.count_tokens(
            model=self.model_name, contents=contents
        )
        print("Total input tokens: ", total_tokens)
        
        response = client.models.generate_content(
            model=self.model_name,
            contents=contents,
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=-1),
                seed=self.seed
            )
        )
        print("Received refinement response.")
        # print(f"Refinement Response: {response.text}")
        return response.text


def run_one_prediction(i, j, base_path, initial_yaml_filepath, model_name, user_prompt, refine_prompt, refine_times, LLM_prediction_ensemble, seed_base):
    """Runs the whole prediction+refinement pipeline for ensemble member j of scene i."""
    llm = LLMCompletion(
        base_path=base_path,
        file_to_analyze=None,
        model_name=model_name,
        user_prompt=user_prompt,
        seed=seed_base + j
    )

    scene_graph_num = i * LLM_prediction_ensemble + j
    print(f"[Scene number: {i} Ensemble prediction number: {j}  Graph_id: {scene_graph_num}] Starting")

    images_dir = os.path.join(base_path, f"scene_slices_{i}_{j}")
    new_yaml_filename = f"habitat_scene_graph_new_graph{scene_graph_num}.yaml"
    new_yaml_filepath = os.path.join(base_path, new_yaml_filename)

    # 1) Create original scene visualization
    create_scene_slices(initial_yaml_filepath, scene_slice_folder_name=images_dir)

    # 2) Generate initial new items
    start_time = time.time()
    generated_response = llm.generate_completion_response(initial_yaml_filepath, images_dir)
    print(f"[Scene number: {i}  Ensemble prediction number: {j}  Graph_id: {scene_graph_num}] Generation took {time.time() - start_time:.2f}s")

    # Parse the response to create the new YAML
    llm.parse_response(generated_response, initial_yaml_filepath, new_yaml_filename)

    # 3) Create updated scene visualization
    create_scene_slices(new_yaml_filepath, scene_slice_folder_name=images_dir)

    # 4) Refinements on LLM's predictions
    refined_response = generated_response
    for r in range(refine_times):
        print(f"[Scene number: {i}  Ensemble prediction number: {j}  Graph_id: {scene_graph_num}] Refinement {r+1}/{refine_times}")
        start_time = time.time()
        refined_response = llm.generate_refinement_response(
            refine_prompt, refined_response, initial_yaml_filepath, images_dir
        )
        print(f"[Scene number: {i}  Ensemble prediction number: {j}  Graph_id: {scene_graph_num}] Refinement took {time.time() - start_time:.2f}s")
        llm.parse_response(refined_response, initial_yaml_filepath, new_yaml_filename)
        create_scene_slices(new_yaml_filepath, scene_slice_folder_name=images_dir)

    print(f"[Scene number: {i}  Ensemble prediction number: {j}  Graph_id: {scene_graph_num}] Done")

if __name__ == "__main__":
    # Completion prompt
    user_prompt = """
    You are a scene expansion assistant that adds plausible new objects and rooms to an existing spatial layout.

    Context:
    - Input includes:
        1. YAML scene graph of current layout.
        2. Visual images matching the YAML.
    - Node types: object, room, nothing (confirmed empty), structure (impassable barrier like a wall).
    - Units: meters (m).

    Goal:
    - Add as many new objects and rooms as are semantically and geometrically plausible. Place new objects in both existing and new rooms.
    - Create edges between:
        - New objects and rooms
        - New rooms and rooms
    - Do NOT connect existing objects to existing rooms.
    - Ensure all additions are semantically and geometrically logical.

    Rules:
    1. Output one YAML code block with ONLY new items.
    2. Structure nodes (Wall, Curtain, Window, Blind) block placement unless a door exists.
    3. New objects must NOT overlap with:
        - nothing nodes
        - structure nodes
        - existing object nodes
    4. When adding a new room, position it logically relative to adjacent rooms and their objects, ensuring the new room's own objects are not placed unusually close to objects in another room.
    5. Do not create new nothing, structure, or door nodes. Scene has no outdoor space or dining room.
    6. Follow exact YAML schema as the input.
    """


    # Refinement prompt
    refine_prompt = """
    You are a scene integration assistant that refines and places new elements into an existing scene graph.

    Context:
    - 'original_scene': the existing scene graph.
    - 'new_items': proposed additions to the scene.
    - Images: horizontal slices of the combined scene (original_scene + new_items).

    Goal:
    Integrate 'new_items' into the scene logically and consistently, making necessary adjustments, deletions and additions.

    Steps:
    1. Refine Attributes:
        - For every node in 'new_items', adjust 'position', 'orientation', and 'dimension' for logical placement. Room nodes do not have 'dimension' or 'orientation'.

    2. Prune and Expand Scene:
        - Delete nodes that are nonsensical or cannot be placed plausibly.
        - Add complementary nodes where needed for realism.
        - Update edges in 'new_items' for semantic and geometric consistency with 'original_scene'.
        - Ensure each room connects to at least one other room to indicate semantic adjacency.

    Rules:
    1. Structure nodes (Wall, Curtain, Window, Blind) are impassable if there is no door on them. Items cannot be placed across them unless a 'door' exists.
    2. Do not create 'nothing', 'structure', or 'door' nodes. Scene has no outdoor space or dining room.
    3. Any object node in the final 'new_items' (original or added) must NOT overlap with:
        - nothing nodes
        - structure nodes
        - existing object nodes
    4. Any room node in the final 'new_items' (original or added) should be positioned logically relative to adjacent rooms and their objects, ensuring the room's own objects are not placed unusually close to objects in another room.
    5. All units are in meters (m).
    6. Output only the final refined 'new_items' as a single valid YAML code block.
    """

    base_path = "/home/apple/Work/scene_graph_processing/Summer_research/Scene_Graph_Ensemble_4/"
    # How many scene graph is created in the pipeline
    scene_graph_created = 2
    # How many LLM predictions are made for each scene graph
    LLM_prediction_ensemble = 4
    # Times to refine each prediction
    refine_times = 2
    # Set seed for LLM for reproducibility
    seed = 42

    llm_pre = LLMCompletion(
            base_path=base_path,
            file_to_analyze=None,
            model_name="gemini-2.5-pro",
            user_prompt=user_prompt,
            seed=seed
        )

    # Total scene graph predicted is scene_graph_created * LLM_prediction_ensemble
    for i in range(scene_graph_created):
        # Specify the basic parameters
        dsg_filename = f"graph{i}_dsg.json"
        original_yaml_filename = f"habitat_scene_graph_original_graph{i}.yaml"
        node_mapping_filename = f"node_mapping_graph{i}.yaml"
        llm_pre.file_to_analyze = os.path.join(base_path, dsg_filename)
        llm_pre.preprocess_file()
        initial_yaml_filepath = os.path.join(base_path, original_yaml_filename)

        max_workers = min(LLM_prediction_ensemble, 4)

        success = True
        with ProcessPoolExecutor(max_workers=max_workers, mp_context=mp.get_context("spawn")) as ex:
            futures = [
                ex.submit(
                    run_one_prediction,
                    i, j, base_path, initial_yaml_filepath,
                    "gemini-2.5-pro", user_prompt, refine_prompt, refine_times, LLM_prediction_ensemble, seed
                )
                for j in range(LLM_prediction_ensemble)
            ]
            for fut in as_completed(futures):
                try:
                    fut.result()
                except Exception as e:
                    print(f"[Scene number: {i}] A job failed: {e}")
                    success = False
        if success:
            print(f"[Scene number: {i}] All jobs completed")
        else:
            print(f"[Scene number: {i}] Some jobs failed")
            break